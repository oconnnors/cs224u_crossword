{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l9kBXR1nt5Sj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT6udpGVoME3",
        "outputId": "33a23e3a-7301-4dbb-c7ff-34b81c71abb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/dom/Desktop/CS224u/cs224u_crossword\n"
          ]
        }
      ],
      "source": [
        "%cd /Users/dom/Desktop/CS224u/cs224u_crossword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMg2suY1sWTd",
        "outputId": "e56bcda5-bd68-4d84-9dd7-b9fd8bd465b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'decrypt-main/'\n",
            "/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq\n"
          ]
        }
      ],
      "source": [
        "%cd decrypt-main/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzBZ9KI_rlax",
        "outputId": "4757119a-c137-43b5-c378-cbf73dbdb165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.1.1-cp38-cp38-macosx_10_13_x86_64.whl (8.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.5 MB 2.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from bs4->-r requirements.txt (line 1)) (4.9.3)\n",
            "Collecting joblib>=1.0.0\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 1)) (2.0.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=b4db1542c09d31208e2dbef7adc8375de256320c3f5beb0285b649b07cd02f7f\n",
            "  Stored in directory: /Users/dom/Library/Caches/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4, joblib, threadpoolctl, scikit-learn, tqdm\n",
            "Successfully installed bs4-0.0.1 joblib-1.1.0 scikit-learn-1.1.1 threadpoolctl-3.1.0 tqdm-4.64.0\n",
            "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VY1hB_6tNot",
        "outputId": "2e3c58fe-92de-4cf1-e5c1-3da6b14c2532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  disjoint.json.zip\n",
            "  inflating: disjoint.json           \n",
            "\n",
            "Archive:  naive_random.json.zip\n",
            "  inflating: naive_random.json       \n",
            "\n",
            "Archive:  disjoint_word_init.json.zip\n",
            "  inflating: disjoint_word_init.json  \n",
            "\n",
            "Archive:  guardian_2020_10_08.json.zip\n",
            "  inflating: guardian_2020_10_08.json  \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ],
      "source": [
        "!pushd ./data && unzip \"*.json.zip\" && popd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X1cxG4JjtqzD"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive/github/cs224u_crossword/decrypt-main/decrypt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10Dl0Xhougrl",
        "outputId": "119afaf0-1748-4c08-be23-3d950c3497e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/github/cs224u_crossword/decrypt-main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/github/cs224u_crossword/decrypt-main/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__swuCCatV38",
        "outputId": "480d784c-fee3-4961-89ba-00dd292a0162"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:decrypt.scrape_parse.guardian_load:Loading splits directly from given json files. Using /Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/data/naive_random.json\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142380/142380 [00:00<00:00, 942103.07it/s]\n",
            "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
            "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
            "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [85428, 28476, 28476]\n",
            "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
            "\t[CleanGuardianClue(clue='Suffering to grasp edge of plant', lengths=[8], soln='agrimony', soln_with_spaces='agrimony', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Chifonie', orig_lengths='8', lengths_punctuation=set()), CleanGuardianClue(clue='Honour Ben and Noel with new order', lengths=[7], soln='ennoble', soln_with_spaces='ennoble', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='7', lengths_punctuation=set()), CleanGuardianClue(clue='Bit the royal we love? Cheers!', lengths=[4], soln='iota', soln_with_spaces='iota', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Screw', orig_lengths='4', lengths_punctuation=set())]\n"
          ]
        }
      ],
      "source": [
        "from decrypt.scrape_parse import (\n",
        "  load_guardian_splits,               # naive random split\n",
        "  load_guardian_splits_disjoint,      # answer-disjoint split\n",
        "  load_guardian_splits_disjoint_hash  # word-initial disjoint split\n",
        ")\n",
        "from decrypt.scrape_parse.guardian_load import SplitReturn\n",
        "\"\"\"\n",
        "each of these methods returns a tuple of `SplitReturn`\n",
        "- soln to clue map (string to List of clues mapping to that soln): Dict[str, List[BaseClue]\n",
        "this enables seeing all clues associated with a given answer word\n",
        "- list of all clues (List[BaseClue])\n",
        "- Tuple of three lists (the train, val, test splits), each is List[BaseClue]\n",
        "\n",
        "Note that\n",
        "load_guardian_splits() will verify that\n",
        "- total glob length matches the one in paper (ie. number of puzzles downloaded matches)\n",
        "- total clue set length matches the one in paper (i.e. filtering is the same)\n",
        "- one of the clues in our train set matches our train set (i.e. a single clue\n",
        "spot check for randomness)\n",
        "If you get an assertion error or an exception during load, please file an\n",
        "issue, since the splits should be identical\n",
        "Alternatively, if you don't care, you can pass `verify=False` to\n",
        "`load_guardian_splits`\n",
        "\"\"\"\n",
        "\n",
        "soln_to_clue_map, all_clues_list, (train, val, test) = load_guardian_splits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5xzCItm77OL"
      },
      "source": [
        "# Baseline t5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT9W0VEe8HGt",
        "outputId": "9efbc55a-b1f2-4350-9155-919343a1f81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'baselines'\n",
            "/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq\n"
          ]
        }
      ],
      "source": [
        "%cd baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDiYWMvJ_F7G",
        "outputId": "83450139-aeee-4276-c234-49656961fa1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%apt` not found.\n"
          ]
        }
      ],
      "source": [
        "%brew install enchant\n",
        "%pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG1NJGwW8ESm",
        "outputId": "d74c2997-880f-48f0-dcc9-29478ce2143b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:No gsheets writer is configured\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from decrypt.scrape_parse import (\n",
        "    load_guardian_splits,\n",
        "    load_guardian_splits_disjoint,\n",
        "    load_guardian_splits_disjoint_hash\n",
        ")\n",
        "\n",
        "import os\n",
        "from decrypt import config\n",
        "from decrypt.common import validation_tools as vt\n",
        "from decrypt.common.util_data import clue_list_tuple_to_train_split_json\n",
        "import logging\n",
        "logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "k_json_folder = config.DataDirs.Guardian.json_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-ev1lRX8M1H",
        "outputId": "551506d0-f01e-4b77-eebd-4664a4d56e6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:decrypt.scrape_parse.guardian_load:Loading splits directly from given json files. Using /Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/data/naive_random.json\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142380/142380 [00:00<00:00, 1168725.82it/s]\n",
            "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
            "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
            "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [85428, 28476, 28476]\n",
            "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
            "\t[CleanGuardianClue(clue='Suffering to grasp edge of plant', lengths=[8], soln='agrimony', soln_with_spaces='agrimony', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Chifonie', orig_lengths='8', lengths_punctuation=set()), CleanGuardianClue(clue='Honour Ben and Noel with new order', lengths=[7], soln='ennoble', soln_with_spaces='ennoble', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='7', lengths_punctuation=set()), CleanGuardianClue(clue='Bit the royal we love? Cheers!', lengths=[4], soln='iota', soln_with_spaces='iota', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Screw', orig_lengths='4', lengths_punctuation=set())]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85428/85428 [00:00<00:00, 409521.69it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28476/28476 [00:00<00:00, 395880.02it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28476/28476 [00:00<00:00, 407772.59it/s]\n",
            "INFO:decrypt.common.util_data:Source target mapping:\n",
            "\tSuffering to grasp edge of plant (8) => agrimony\n",
            "\n",
            "INFO:decrypt.common.util_data:Finished writing all files\n",
            "INFO:decrypt.scrape_parse.guardian_load:Loading splits directly from given json files. Using /Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/data/disjoint_word_init.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'idx': -1,\n",
            " 'input': 'Suffering to grasp edge of plant (8)',\n",
            " 'target': 'agrimony'}\n",
            "{'idx': -1,\n",
            " 'input': 'Honour Ben and Noel with new order (7)',\n",
            " 'target': 'ennoble'}\n",
            "{'idx': -1, 'input': 'Bit the royal we love? Cheers! (4)', 'target': 'iota'}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142380/142380 [00:00<00:00, 1244415.42it/s]\n",
            "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
            "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
            "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [75847, 32628, 33905]\n",
            "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
            "\t[CleanGuardianClue(clue='Sailor boy in his hammock', lengths=[4], soln='abed', soln_with_spaces='abed', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='4', lengths_punctuation=set()), CleanGuardianClue(clue='With a degree, I leave this subject', lengths=[5], soln='maths', soln_with_spaces='maths', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='5', lengths_punctuation=set()), CleanGuardianClue(clue='Burrow to cure limb and make sure one gets up', lengths=[3, 3, 5], soln='setthealarm', soln_with_spaces='set the alarm', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Araucaria', orig_lengths='3,3,5', lengths_punctuation={','})]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75847/75847 [00:00<00:00, 401504.13it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32628/32628 [00:00<00:00, 394048.18it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33905/33905 [00:00<00:00, 394396.32it/s]\n",
            "INFO:decrypt.common.util_data:Source target mapping:\n",
            "\tSailor boy in his hammock (4) => abed\n",
            "\n",
            "INFO:decrypt.common.util_data:Finished writing all files\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'idx': -1, 'input': 'Sailor boy in his hammock (4)', 'target': 'abed'}\n",
            "{'idx': -1,\n",
            " 'input': 'With a degree, I leave this subject (5)',\n",
            " 'target': 'maths'}\n",
            "{'idx': -1,\n",
            " 'input': 'Burrow to cure limb and make sure one gets up (3,3,5)',\n",
            " 'target': 'set the alarm'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def make_dataset(split_type: str, overwrite=True):\n",
        "    assert split_type in ['naive_random', 'naive_disjoint', 'word_init_disjoint']\n",
        "    if split_type == 'naive_random':\n",
        "        load_fn = load_guardian_splits\n",
        "        tgt_dir = config.DataDirs.DataExport.guardian_naive_random_split\n",
        "    elif split_type == 'naive_disjoint':\n",
        "        load_fn = load_guardian_splits_disjoint\n",
        "        tgt_dir = config.DataDirs.DataExport.guardian_naive_disjoint_split\n",
        "    else:\n",
        "        load_fn = load_guardian_splits_disjoint_hash\n",
        "        tgt_dir = config.DataDirs.DataExport.guardian_word_init_disjoint_split\n",
        "\n",
        "    _, _, (train, val, test) = load_fn(k_json_folder)\n",
        "\n",
        "    os.makedirs(tgt_dir, exist_ok=True)\n",
        "    # write the output as json\n",
        "    try:\n",
        "        clue_list_tuple_to_train_split_json((train, val, test),\n",
        "                                            comment=f'Guardian data. Split: {split_type}',\n",
        "                                            export_dir=tgt_dir,\n",
        "                                            overwrite=overwrite)\n",
        "    except FileExistsError:\n",
        "        logging.warning(f'You have already generated the {split_type} dataset.\\n'\n",
        "                        f'It is located at {tgt_dir}\\n'\n",
        "                        f'To regenerate, pass overwrite=True or delete it\\n')\n",
        "\n",
        "\n",
        "make_dataset('naive_random')\n",
        "make_dataset('word_init_disjoint')\n",
        "# you can also make_dataset('naive_disjoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zGWP4avE1LB",
        "outputId": "7da9f3be-87c0-49b2-c3f7-4f687db98e67"
      },
      "outputs": [],
      "source": [
        "%pip install transformers==4.4.2\n",
        "%pip install wandb\n",
        "%pip install torch==1.7.1\n",
        "%pip install torchvision==0.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfY_llgXHfHd",
        "outputId": "d4c415e0-d098-41ac-be1a-f01a18489019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq\n"
          ]
        }
      ],
      "source": [
        "%cd /Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLR5rbehZYeQ",
        "outputId": "07075e34-4ea1-419e-f002-788b788efbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting overrides\n",
            "  Downloading overrides-6.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting typing-utils>=0.0.3\n",
            "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: typing-utils, overrides\n",
            "Successfully installed overrides-6.1.0 typing-utils-0.1.0\n",
            "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import typing\n",
        "import abc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1jv42879) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">base_line_naive</strong>: <a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/1jv42879\" target=\"_blank\">https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/1jv42879</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220602_145912-1jv42879/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1jv42879). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/wandb/run-20220602_145929-wgzeixmg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/wgzeixmg\" target=\"_blank\">baseline_naive</a></strong> to <a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/wgzeixmg?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x122062d60>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"baseline\", entity=\"224u-s22-cryptic-crosswords\",name=\"baseline_naive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62_requirements.txt README.md           \u001b[34mdecrypt-main\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIEbCh9YG_Ba",
        "outputId": "b30dd294-9e0f-4035-c163-71f9c3c98436"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1wbiihk9) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">baseline_naive</strong>: <a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/1wbiihk9\" target=\"_blank\">https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/1wbiihk9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220602_162536-1wbiihk9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1wbiihk9). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/wandb/run-20220602_162647-3n8z5l3g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/3n8z5l3g\" target=\"_blank\">baseline_naive</a></strong> to <a href=\"https://wandb.ai/224u-s22-cryptic-crosswords/baseline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_naive\n"
          ]
        }
      ],
      "source": [
        "#/usr/bin/env python3\n",
        "import wandb\n",
        "\n",
        "run = wandb.init(project=\"baseline\", entity=\"224u-s22-cryptic-crosswords\",name=\"baseline_naive\")\n",
        "print(run.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/224ufinal/lib/python3.9/site-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/224ufinal/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AlbertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.7837321758270264"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# the forward function automatically creates the correct decoder_input_ids\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdterr\u001b[0m (\u001b[33m224u-s22-cryptic-crosswords\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/wandb/run-20220603_154157-2kv6fa2e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbaseline_naive\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/baseline\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/2kv6fa2e\u001b[0m\n",
            "WARNING:common_seq.util:Logger had handlers already set WTF\n",
            "..... CLEARING\n",
            "[06.03 15:42:03] [train_clues.py:132 - <module>()]\t train_clues.py --model_name=t5-small --default_train=base --name=baseline_naive --project=baseline --wandb_dir=./wandb --data_dir=../data/clue_json/guardian/naive_random\n",
            "[06.03 15:42:03] [util.py:160 - set_seed()]\t Setting seed\n",
            "[06.03 15:42:03] [util_checkpoint.py:65 - __init__()]\t Saver will track (metric, maximize?)\n",
            " [('dev/num_match_top_sampled', True), ('epoch', True)]\n",
            "[06.03 15:42:03] [util.py:78 - get_available_devices()]\t Device: cpu\t GPU IDs: []\t machine: Dominics-Officejet-Pro-8900.local\n",
            "\n",
            "[06.03 15:42:10] [util_dataloader_batch.py:56 - __init__()]\t Loading cluedatasetbatched of type train\n",
            "[06.03 15:42:10] [util_dataloader_batch.py:72 - __init__()]\t For dataset, found readme: \n",
            "[06.03 15:42:10] [util_dataloader_batch.py:73 - __init__()]\t ['Guardian data. Split: naive_random\\n', 'Total: 142380\\n', 'splits: [85428, 28476, 28476]\\n', '\\n', \"{'idx': -1,\\n\", \" 'input': 'Suffering to grasp edge of plant (8)',\\n\", \" 'target': 'agrimony'}\\n\", \"{'idx': -1,\\n\", \" 'input': 'Honour Ben and Noel with new order (7)',\\n\", \" 'target': 'ennoble'}\\n\", \"{'idx': -1, 'input': 'Bit the royal we love? Cheers! (4)', 'target': 'iota'}\\n\", '\\n', '\\n']\n",
            "[06.03 15:42:10] [util_dataloader_batch.py:197 - _get_dataloader_batched()]\t Dataset train loaded with size: 85428\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_clues.py\", line 158, in <module>\n",
            "    local_trainer = ClueTrainer(wandb.config, local_rh, aux_config=aux_config)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_clues.py\", line 26, in __init__\n",
            "    super().__init__(config, rh, **kwargs)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 751, in __init__\n",
            "    super().__init__(config, rh, **kwargs)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 260, in __init__\n",
            "    self.setup_dataloaders()\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_clues.py\", line 44, in setup_dataloaders\n",
            "    super().setup_dataloaders()\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 831, in setup_dataloaders\n",
            "    self.setup_dataloaders_no_multi()\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 774, in setup_dataloaders_no_multi\n",
            "    self._get_dataloaders(batched_collate_fns=batched_collate_fcns)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 854, in _get_dataloaders\n",
            "    get_dataloaders_batched(self.tokenizer,\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util_dataloader_batch.py\", line 228, in get_dataloaders_batched\n",
            "    train_loader = _get_dataloader_batched(tokenizer,\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util_dataloader_batch.py\", line 200, in _get_dataloader_batched\n",
            "    return _get_dataloader_from_dataset(tokenizer, data_set, dl_config, inputted_collate_fn)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util_dataloader_batch.py\", line 169, in _get_dataloader_from_dataset\n",
            "    dataloader = ClueDataLoaderBatched(dataset,\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util_dataloader_batch.py\", line 22, in __init__\n",
            "    self.__post_init_check()\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util_dataloader_batch.py\", line 29, in __post_init_check\n",
            "    for idx, batch in enumerate(self):\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 352, in __iter__\n",
            "    return self._get_iterator()\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 294, in _get_iterator\n",
            "    return _MultiProcessingDataLoaderIter(self)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 801, in __init__\n",
            "    w.start()\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/process.py\", line 121, in start\n",
            "    self._popen = self._Popen(self)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/context.py\", line 224, in _Popen\n",
            "    return _default_context.get_context().Process._Popen(process_obj)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n",
            "    return Popen(process_obj)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
            "    super().__init__(process_obj)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n",
            "    self._launch(process_obj)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n",
            "    reduction.dump(process_obj, fp)\n",
            "  File \"/opt/anaconda3/envs/224ufinal/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\n",
            "    ForkingPickler(file, protocol).dump(obj)\n",
            "AttributeError: Can't pickle local object '_get_dataloader_from_dataset.<locals>.curried_collate_fn'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   dev/num_match_in_sample best_in_sample\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: dev/num_match_top_sampled best_top_sampled\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbaseline_naive\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/baseline/runs/2kv6fa2e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/wandb/run-20220603_154157-2kv6fa2e/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train_clues.py --model_name=\"t5-small\" --default_train=base --name=baseline_naive --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Curricular training\n",
        "At this point you should have a files at\n",
        "\n",
        "./data/clue_json/curricular/ACW/train.json\n",
        "./data/clue_json/curricular/anagram/[train.json, anag_indics.json]\n",
        "Running curricular training is the same as running main t5 vanilla train, except that we pass an extra multitask flag, which specifies the curriculum to use. See seq2seq/multitask_config. You should pass one of the names from multi_config dict in that file\n",
        "\n",
        "For example, to train the naive split with the top performing curricular approach (i.e. the result in table 3 that is ACW + ACW-descramble)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdterr\u001b[0m (\u001b[33m224u-s22-cryptic-crosswords\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/wandb/run-20220602_163312-1iy68p1t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnaive_top_curricular\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/curricular\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/curricular/runs/1iy68p1t\u001b[0m\n",
            "WARNING:common_seq.util:Logger had handlers already set WTF\n",
            "..... CLEARING\n",
            "[06.02 16:33:17] [train_clues.py:132 - <module>()]\t train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir=./wandb --data_dir=../data/clue_json/guardian/naive_random --multitask=ACW__ACW_descramble\n",
            "[06.02 16:33:17] [util.py:160 - set_seed()]\t Setting seed\n",
            "[06.02 16:33:17] [util_checkpoint.py:65 - __init__()]\t Saver will track (metric, maximize?)\n",
            " [('dev/num_match_top_sampled', True), ('multisave', True), ('multi/acw/num_match_in_sample', True), ('epoch', True)]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_clues.py\", line 158, in <module>\n",
            "    local_trainer = ClueTrainer(wandb.config, local_rh, aux_config=aux_config)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_clues.py\", line 26, in __init__\n",
            "    super().__init__(config, rh, **kwargs)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 751, in __init__\n",
            "    super().__init__(config, rh, **kwargs)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 255, in __init__\n",
            "    self.setup_model_and_device()       # populate 3 above; potentially add special tokens\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/train_abc.py\", line 377, in setup_model_and_device\n",
            "    self.device, self.gpu_ids = util.get_available_devices(assert_cuda=True)\n",
            "  File \"/Users/dom/Desktop/CS224u/cs224u_crossword/decrypt-main/seq2seq/common_seq/util.py\", line 75, in get_available_devices\n",
            "    raise ValueError('no cuda found')\n",
            "ValueError: no cuda found\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   dev/num_match_in_sample best_in_sample\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: dev/num_match_top_sampled best_top_sampled\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mnaive_top_curricular\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/224u-s22-cryptic-crosswords/curricular/runs/1iy68p1t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/wandb/run-20220602_163312-1iy68p1t/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random' --multitask=ACW__ACW_descramble"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "cs224u_master.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "801dc401bfe477544045a8dc83d63efd6a32868ac2a652a4fbe6d30f82d2d95a"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('224ufinal')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
