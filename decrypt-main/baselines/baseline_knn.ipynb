{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from decrypt import config\n",
    "k_json_folder = config.DataDirs.Guardian.json_folder\n",
    "\n",
    "from decrypt.scrape_parse import (\n",
    "    load_guardian_splits,\n",
    "    load_guardian_splits_disjoint_hash\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from decrypt.common.puzzle_clue import GuardianClue\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from decrypt.common import validation_tools as vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# no need to lowercase - countvectorizer does this\n",
    "def load_data(clue_list: List[GuardianClue], add_lens=False):\n",
    "    \"\"\"\n",
    "    Take clue_list and return the X and Y data\n",
    "    \"\"\"\n",
    "    def iter_fcn(clue: GuardianClue):\n",
    "        if add_lens:\n",
    "            ret = clue.clue_with_lengths(\"|\")\n",
    "        else:\n",
    "            ret = clue.clue\n",
    "        return ret\n",
    "\n",
    "    X = [iter_fcn(c) for c in clue_list]\n",
    "    Y = [c.soln_with_spaces.lower() for c in clue_list]\n",
    "    return X, Y\n",
    "\n",
    "def knn_eval(train, val, add_lens, knn_neighbors: int, verify=False):\n",
    "    \"\"\"\n",
    "    :param add_lens: whether to add lengths to the input clues\n",
    "    :param knn_neighbors: number of neighbors to use when doing \"beam search\". None => no beam search\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    train_inputs, train_targets = load_data(train, add_lens)\n",
    "    test_inputs, test_targets = load_data(val, add_lens)\n",
    "    print(train_inputs[:2])\n",
    "\n",
    "    # set up the bag-of-words vectorizer\n",
    "    # token patter needed for the length specification\n",
    "    bow_vectorizer = CountVectorizer(token_pattern='[a-z\\d()|]+',\n",
    "                                     ngram_range=(1,1))     # further ngrams degrade performance\n",
    "    bowVect = bow_vectorizer.fit(train_inputs)\n",
    "\n",
    "    # show that everything was vectorized correctly\n",
    "    print(len(bowVect.vocabulary_))\n",
    "    if verify:\n",
    "        for w in train_inputs[0].replace(\",\",\" \").lower().split(\" \"):\n",
    "            if w == '': continue\n",
    "            print(bowVect.vocabulary_[w])\n",
    "\n",
    "    bowTrain = bowVect.transform(train_inputs)\n",
    "    bowTest = bowVect.transform(test_inputs)\n",
    "\n",
    "    # fit KNN\n",
    "    # neighbor setting here doesn't matter; can put in call to knn.kneighbors\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(bowTrain, train_targets)\n",
    "\n",
    "    # predict (runs long)\n",
    "    # get the nearest neighbors (beam search)\n",
    "    # this returns in sorted order, so commented code not needed\n",
    "    # nn_dist, nn_idx = knn.kneighbors(bowTest, n_neighbors=knn_neighbors, return_distance=True)\n",
    "    # nn_dist_and_idx = zip(nn_dist, nn_idx)\n",
    "    nn = knn.kneighbors(bowTest, n_neighbors=knn_neighbors, return_distance=False)\n",
    "\n",
    "    # get the predictions (\"greedy\")\n",
    "    pred = knn.predict(bowTest)\n",
    "    return train_targets, test_targets, nn, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_knn(val_set: List[GuardianClue],\n",
    "             train_targets,\n",
    "             test_targets,\n",
    "             nn,\n",
    "             pred):\n",
    "    model_outputs = []\n",
    "\n",
    "    # don't need to check idx set since we have a 1:1 of val_gc to test_tgt\n",
    "    # nn_list is already sorted\n",
    "    for val_gc, test_tgt, nn_list, greedy_pred in tqdm(zip(val_set, test_targets, nn, pred)):\n",
    "        assert val_gc.soln_with_spaces == test_tgt\n",
    "        neighbor_solns = [train_targets[n] for n in nn_list]\n",
    "\n",
    "        # nbr set is the list of indices of nearest neighbor\n",
    "        # we retrieve all the solns for those neighbors (y_train[i])\n",
    "        mp = vt.ModelPrediction(idx=val_gc.idx,\n",
    "                            input=val_gc.clue_with_lengths(punct=\"|\"),\n",
    "                            target=test_tgt,\n",
    "                            greedy=greedy_pred,\n",
    "                             sampled=neighbor_solns)\n",
    "\n",
    "        mp.model_eval = vt.eval(mp)\n",
    "        model_outputs.append(mp)\n",
    "\n",
    "    return model_outputs\n",
    "\n",
    "def aggregate(val, output_tuple):\n",
    "    model_out = eval_knn(val, *output_tuple)\n",
    "    vt.all_aggregate(model_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eval on naive val set (with/without lens)\n",
    "def run_eval_knn(val_or_test: str, naive_or_disj: str, nn=3000):\n",
    "    assert val_or_test in [\"val\", \"test\"]\n",
    "    assert naive_or_disj in [\"naive\", \"disj\"]\n",
    "    if naive_or_disj == \"naive\":\n",
    "        load_fn = load_guardian_splits\n",
    "    else:\n",
    "        load_fn = load_guardian_splits_disjoint_hash\n",
    "    _, _, (train_local, val, test) = load_fn(k_json_folder)\n",
    "    if val_or_test == \"val\":\n",
    "        val_local = val\n",
    "    else:\n",
    "        val_local = test\n",
    "\n",
    "    knn_tuple_random_val_nolens = knn_eval(train_local, val_local, add_lens=False, knn_neighbors=nn)\n",
    "    aggregate(val_local, knn_tuple_random_val_nolens)\n",
    "\n",
    "    knn_tuple_random_val_lens = knn_eval(train_local, val_local, add_lens=True, knn_neighbors=nn)\n",
    "    aggregate(val_local, knn_tuple_random_val_lens)\n",
    "\n",
    "    return knn_tuple_random_val_nolens, knn_tuple_random_val_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run with nn=3000 to replicate research\n",
    "knn_tuple_random_val_nolens, knn_tuple_random_val_lens = run_eval_knn(val_or_test=\"val\",\n",
    "                                                                      naive_or_disj=\"naive\",\n",
    "                                                                      nn=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To reproduce the two rows corresponding to KNN in Main Results, also run with\n",
    "- val_or_test = \"test\"\n",
    "- naive_or_disj=\"disj\"\n",
    "\n",
    "Note that for the Main Results Table 2, the metrics we include in the table correspond to\n",
    "- `agg_top_match`\n",
    "- `agg_top_10_after_filter`\n",
    "\n",
    "More details of these metric calculations can be found in `decrypt.common.validation_tools`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Supplementary to verify KNN works the way we expect\n",
    "###\n",
    "\n",
    "## verify how KNN does tokenization\n",
    "train_inputs, train_targets = load_data(train, True)\n",
    "print(train_inputs[:2])\n",
    "\n",
    "# set up the bag-of-words vectorizer\n",
    "# token patter needed for the length specification\n",
    "# punctuation not included in token pattern (e.g. , or ') will be split and treated as space\n",
    "# see below experiment\n",
    "bow_vectorizer = CountVectorizer(token_pattern='[a-z\\d()|]+',\n",
    "                                 ngram_range=(1,1))     # further ngrams degrade performance\n",
    "bowVect = bow_vectorizer.fit(train_inputs)\n",
    "\n",
    "# show that everything was vectorized correctly\n",
    "print(len(bowVect.vocabulary_))\n",
    "print()\n",
    "# for w in train_inputs[0].replace(\",\",\" \").lower().split(\" \"):\n",
    "# need to replace any punct that occurs\n",
    "all = []\n",
    "for idx, w in enumerate(train_inputs[12].replace(\"'\", \"\").lower().split(\" \")):\n",
    "    print(w)\n",
    "    try:\n",
    "        val = bowVect.vocabulary_[w]\n",
    "        print(val)\n",
    "        all.append(val)\n",
    "    except:\n",
    "        pass\n",
    "print(sorted(all))\n",
    "\n",
    "print(train_inputs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "matrix = bow_vectorizer.transform([train_inputs[12]])\n",
    "print(matrix)\n",
    "for i,j in matrix:\n",
    "    print(bowVect.vocabulary_[])\n",
    "\n",
    "bow_vectorizer.inverse_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, c in enumerate(train):\n",
    "    if len(c.lengths) > 1:\n",
    "        print(c.idx)\n",
    "        print(i)\n",
    "        break\n",
    "print(train_inputs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
