{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For running the T5 vanilla baseline\n",
    "Here we provide code to\n",
    "1. Setup datafiles for running t5 (i.e. produce json files)\n",
    "1. Run the seq2seq model to produce outputs, saving in a directory that matches k_t5_outputs below\n",
    "    - Train model (produces saved checkpoints)\n",
    "    - Eval top performing model (load from top checkpoint, produces json outputs)\n",
    "\n",
    "1. run eval code using the json output from model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from decrypt.scrape_parse import (\n",
    "    load_guardian_splits,\n",
    "    load_guardian_splits_disjoint,\n",
    "    load_guardian_splits_disjoint_hash\n",
    ")\n",
    "\n",
    "import os\n",
    "from decrypt import config\n",
    "from decrypt.common import validation_tools as vt\n",
    "from decrypt.common.util_data import clue_list_tuple_to_train_split_json\n",
    "import logging\n",
    "logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "k_json_folder = config.DataDirs.Guardian.json_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Produce datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(split_type: str, overwrite=False):\n",
    "    assert split_type in ['naive_random', 'naive_disjoint', 'word_init_disjoint']\n",
    "    if split_type == 'naive_random':\n",
    "        load_fn = load_guardian_splits\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_naive_random_split\n",
    "    elif split_type == 'naive_disjoint':\n",
    "        load_fn = load_guardian_splits_disjoint\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_naive_disjoint_split\n",
    "    else:\n",
    "        load_fn = load_guardian_splits_disjoint_hash\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_word_init_disjoint_split\n",
    "\n",
    "    _, _, (train, val, test) = load_fn(k_json_folder)\n",
    "\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "    # write the output as json\n",
    "    try:\n",
    "        clue_list_tuple_to_train_split_json((train, val, test),\n",
    "                                            comment=f'Guardian data. Split: {split_type}',\n",
    "                                            export_dir=tgt_dir,\n",
    "                                            overwrite=overwrite)\n",
    "    except FileExistsError:\n",
    "        logging.warning(f'You have already generated the {split_type} dataset.\\n'\n",
    "                        f'It is located at {tgt_dir}\\n'\n",
    "                        f'To regenerate, pass overwrite=True or delete it\\n')\n",
    "\n",
    "\n",
    "make_dataset('naive_random')\n",
    "make_dataset('word_init_disjoint')\n",
    "# you can also make_dataset('naive_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Running (training) the model\n",
    "1. Setup environment\n",
    "    1. You should setup wandb for logging (that's where metrics will show up).\n",
    "    If you try to run without wandb, then wandb will tell you what you need to do to initialize\n",
    "\n",
    "    1. The relevant libraries used for our runs are\n",
    "        - transformers==4.4.2\n",
    "        - wandb==0.10.13 # this can probably be updated\n",
    "        - torch==1.7.1+cu110\n",
    "        - torchvision==0.8.2+cu110\n",
    "    - Choose place for your wandb dir, e.g., `'./wandb' `\n",
    "1. Train the model\n",
    "    1. Note that the default arguments are given in args_cryptic. See `--default_train` and `--default_val`\n",
    "    - Note that, when looking at logging messages or wandb, it will appear that epochs start at 11.\n",
    "    This is done so that we have \"space\" for 10 \"warmup\" epochs for curricular training.\n",
    "     This space causes all plots in wandb to line up.\n",
    "    1. from directory seq2seq, run the commands in the box below.\n",
    "     This will produce model checkpoints that can then be used for evaluation.\n",
    "\n",
    "\n",
    "\n",
    "Baseline naive\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_naive --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random'\n",
    "```\n",
    "Baseline (naive split), without lengths\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_naive_nolens --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint' --special=no_lens\n",
    "```\n",
    "Baseline disjoint (word initial disjoint)\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_disj --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint'\n",
    "```\n",
    "Baseline disjoint (word initial disjoint), without lengths\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_disj --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint' --special=no_lens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Evaluating the model\n",
    "During training we generate only 5 beams for efficiency. For eval we generate 100.\n",
    "1. Select the best model based on num_match_top_sampled. There should be\n",
    "a logging statement at the end of the run that prints the location\n",
    "of the best model checkpoint.\n",
    "You can also find it by matching the peak in the wandb.ai metrics graph\n",
    "to the appropriate model save.\n",
    "2. Run eval using that model (see commands below), which will\n",
    "produce a file in a (new, different) wandb directory that looks like `epoch_11.pth.tar.preds.json` (i.e only a single epoch)\n",
    "\n",
    "For example,\n",
    "\n",
    "Baseline naive, if epoch 20 is best (you'll need to set the run_name)\n",
    "This produces generations for the validation set\n",
    "```python\n",
    "python train_clues.py --default_val=base --name=baseline_naive_val --project=baseline --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_20.pth.tar\n",
    "```\n",
    "\n",
    "To produce generations for the test set,\n",
    "```python\n",
    "python train_clues.py --default_val=base --name=baseline_naive_val --project=baseline --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_10.pth.tar --test\n",
    "```\n",
    "\n",
    "This should also be run for the no-lengths versions if you want to replicate those results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we produce metrics by evaluating the json that was produced\n",
    "1. Change the k_t5_outputs_dir value to the location where you have saved the json files. \n",
    "    - Recommend copying all of the preds.json files into a common directory and working from that.\n",
    "    - Alternatively you could modify the code below and pass in a full path name to each of the json outputs (using the wandb directory path)\n",
    "1. For each t5 model eval (above) that you ran (each of which produced some `..preds.json` file, run `load_and_run()` to get metrics for those outputs\n",
    "1. The resulting outputs are the values we report in the tables. See `decrypt/common/validation_tools.ModelEval` for more details about the numbers that are produced. Percentages are prefixed by agg_\n",
    "\n",
    "Note that for the Main Results Table 2, the metrics we include in the table correspond to\n",
    "- `agg_top_match`\n",
    "- `agg_top_10_after_filter`\n",
    "\n",
    "More details of these metric calculations can be found in `decrypt.common.validation_tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for example, if your output files are in\n",
    "# 'decrypt/t5_outputs/'\n",
    "# and you will run the below, e.g., if you have named the files\n",
    "# baseline_naive_e12_test.json\n",
    "# (.json will be appended for you by the load_and_run_t5 function)\n",
    "# a better name for load_and_run is load_and_eval\n",
    "\n",
    "# for example\n",
    "### primary - test\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_e12_test')\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_nolens_e15_test')     # test set\n",
    "\n",
    "## primary val\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_e12_val')\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_nolens_e15_val')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
