{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up for curricular experiment\n",
    "\n",
    "This assumes you have already followed the instructions in `baselines/baseline_t5`, which will set up the baseline clue files for model input\n",
    "\n",
    "### Datasets\n",
    "1. Download and unzip the xd cw crossword set from http://xd.saul.pw/xd-clues.zip.\n",
    "    - Save it as './data/original/xd/clues.tsv'\n",
    "2. Preprocess the dataset using this notebook\n",
    "3. The dataset will be saved to k_acw_export_dir (as a single train.json file)\n",
    "4. We will also produce the anagram dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from decrypt.scrape_parse.acw_load import get_clean_xd_clues\n",
    "from decrypt import config\n",
    "from decrypt.common.util_data import clue_list_tuple_to_train_split_json\n",
    "from decrypt.common import validation_tools as vt\n",
    "\n",
    "k_xd_orig_tsv = config.DataDirs.OriginalData.k_xd_cw        # ./data/original/xd/clues.tsv\n",
    "k_acw_export_dir = config.DataDirs.DataExport.xd_cw_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defaults to strip periods, remove questions, remove abbrevs, remove fillin\n",
    "stc_map, all_clues = get_clean_xd_clues(k_xd_orig_tsv,\n",
    "                                        remove_if_not_in_dict=False,\n",
    "                                        do_filter_dupes=True)\n",
    "clue_list_tuple_to_train_split_json((all_clues,),\n",
    "                                    comment='ACW set; xd cw set, all',\n",
    "                                    export_dir=k_acw_export_dir,\n",
    "                                    overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# produce anagram datasets\n",
    "# roughly 3 minutes to complete\n",
    "from decrypt.common import anagrammer\n",
    "anagrammer.gen_db_with_both_inputs(update_flag=\"overwrite\")\n",
    "\n",
    "from decrypt.common.util_data import (\n",
    "    get_anags,\n",
    "    write_json_tuple\n",
    ")\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_anag_sets_json():\n",
    "    all_anags = get_anags(max_num_words=-1)\n",
    "    json_list = []\n",
    "    for idx, a_list in enumerate(all_anags):\n",
    "        json_list.append(dict(idx=idx,\n",
    "                              anag_list=a_list))\n",
    "    print(json_list[0])\n",
    "\n",
    "    # normally would be (idx, input, tgt)\n",
    "    output_tuple = [json_list,]\n",
    "\n",
    "    os.makedirs(config.DataDirs.DataExport.anag_dir)\n",
    "    write_json_tuple(output_tuple,\n",
    "                     comment=\"List of all anagram groupings\",\n",
    "                     export_dir=config.DataDirs.DataExport.anag_dir,\n",
    "                     overwrite=False)\n",
    "\n",
    "def make_anag_indic_list_json():\n",
    "    # make the indicator list\n",
    "    with open(config.DataDirs.OriginalData.k_deits_anagram_list, 'r') as f:\n",
    "        all_anag_indicators = f.readlines()\n",
    "        print(len(all_anag_indicators))\n",
    "\n",
    "    final_indic_list = []\n",
    "    for a in all_anag_indicators:\n",
    "        final_indic_list.append(a.replace('_', \" \").strip())\n",
    "    with open(config.DataDirs.DataExport.anag_indics, 'w') as f:\n",
    "        json.dump(final_indic_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_anag_sets_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_anag_indic_list_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Curricular training\n",
    "1. At this point you should have a files at\n",
    " - `./data/clue_json/curricular/ACW/train.json`\n",
    " - `./data/clue_json/curricular/anagram/[train.json, anag_indics.json]`\n",
    "\n",
    "2. Running curricular training is the same as running main t5 vanilla train, except that we pass an extra multitask flag, which specifies the curriculum to use. See `seq2seq/multitask_config`. You should pass one of the names from  `multi_config` dict in that file\n",
    "\n",
    "For example, to train the naive split with the top performing curricular approach (i.e. the result in table 3 that is ACW + ACW-descramble)\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random' --multitask=ACW__ACW_descramble\n",
    "```\n",
    "\n",
    "Note that the modifications on the dataset are done at the\n",
    "\n",
    "3. To produce Table 3 of the results\n",
    "    -  we don't need to do a model_eval run since the outputted predictions have 5 generations\n",
    "       (which is all we report for that table (for faster experimental iteration).\n",
    "    - we need to run `load_and_run_t5` on all outputs (column 1) and on the anagram subset (column 2)\n",
    "      See below for how we do this.\n",
    "\n",
    "4. For our top result in Table 2 (main resuls) we\n",
    "    1. scale up the curricular period (to 4 total epochs)\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random' --multitask=final_top_result_scaled_up\n",
    "```\n",
    "    2. eval with full 100 generations, as before:\n",
    "e.g., if epoch 10 is best (you'll need to set the run_name)\n",
    "This runs the eval set (change the run_name)\n",
    "```python\n",
    "python train_clues.py --default_val=base --name=curricular_naive_top --project=curricular --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_10.pth.tar\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from decrypt.common.label_anagrams import make_label_set\n",
    "\n",
    "labels = make_label_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# note that this should be run directly on the top model output from curricular training\n",
    "# otherwise (eg. if 100 beams were used), the top 5 output\n",
    "# sequences would be expected to change\n",
    "# remember not to append .json\n",
    "\n",
    "# eval on the full output (5 beams / 5 sequences)\n",
    "# this is column 1 of table 3\n",
    "vt.load_and_run_t5('outputs/model_output.preds',\n",
    "                   # pre_truncate=5,        # should not be needed since we have only 5 outputs\n",
    "                   do_length_filter=True)\n",
    "\n",
    "# run on the anagram subset\n",
    "# this is column 2 of table 3\n",
    "vt.load_and_run_t5('outputs/model_output.preds',\n",
    "                   filter_fcn=vt.make_set_filter(labels, 'anag_direct'),\n",
    "                   # pre_truncate=5,\n",
    "                   do_length_filter=True)\n",
    "\n",
    "# we are looking at agg_top_match (which is after filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}