{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece\n",
    "%pip install transformers==4.4.2\n",
    "%pip install wandb\n",
    "%pip install torch==1.7.1\n",
    "%pip install torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece\n",
    "from transformers import AlbertTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "if (not os.path.exists(\"./data/cryptonite-official-split/cryptonite-train.jsonl\")):\n",
    "    %wget https://github.com/aviaefrat/cryptonite/raw/main/data/cryptonite-official-split.zip\n",
    "    %unzip -d ./data/cryptonite-official-split/ cryptonite-official-split.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "with open('data/cryptonite-official-split/cryptonite-train.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "input_sequences = [] \n",
    "output_sequences = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    input_sequences.append(result['clue'])\n",
    "    output_sequences.append(result['answer'])\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"solve cryptic crossword:\"\n",
    "\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    output_sequences, padding=\"longest\", max_length=max_target_length, truncation=True\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels = torch.tensor(labels)\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
